# Import the required modules
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import time

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
import hvplot.pandas

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost

# Pre-Processing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# Models
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostRegressor
from xgboost import XGBRegressor

# Save off the model/scaler
import pickle

# suppress warnings
import warnings
warnings.filterwarnings('ignore')


# Load the cleaned data
df_cleaned = pd.read_csv('../Resources/cleaned_data.csv')









df_cleaned.info()


df_cleaned['car_age'] = 2025 - df_cleaned['model_year']


# Convert 'Yes' and 'No' to lowercase before mapping
df_cleaned['clean_title'] = df_cleaned['clean_title'].str.lower()

# Map 'yes' to 1 and 'no' to 0
df_cleaned['clean_title'] = df_cleaned['clean_title'].map({'yes': 1, 'no': 0})

# Check the result
df_cleaned['clean_title'].head()


features = ['brand','model_year','car_age', 'mileage', 
            'fuel_type', 'transmission_category', 
            'exterior_color_category', 'accident', 'clean_title', 'price']

df2 = df_cleaned.loc[:, features]
df2.head()



df2.info()


# Define preprocessing for numeric features (model_year, mileage, car_age)
numeric_features = ['model_year', 'mileage', 'car_age']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values with median
    ('scaler', StandardScaler())])  # Scale numerical features

# Define preprocessing for categorical features (brand, fuel_type, transmission_category, exterior_color_category, accident)
categorical_features = ['brand', 'fuel_type', 'transmission_category', 'exterior_color_category', 'accident']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent', missing_values=pd.NA)),  # Fill missing values with most frequent
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encoding for categorical features (return dense)

# Combine preprocessing for numeric and categorical features using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Apply the transformation to the data
preprocessed_X_train = preprocessor.fit_transform(df_cleaned)

# Step 2: Get the number of features in the transformed data
print(f"Shape of transformed data: {preprocessed_X_train.shape}")


# Correlation Analysis (this is optional and not needed for the ML Experiment)
# It just shows what the data looks like after transformation before training
# We will still declare a full pipeline of preprocessing + training

# Use only preprocessing pipeline to transform the data
preprocessed_X_train = preprocessor.fit_transform(df_cleaned)

# Convert preprocessed data to a DataFrame
# Get the feature names after one-hot encoding
encoded_feature_names = (numeric_features +list(preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(categorical_features)))

df_final = pd.DataFrame(preprocessed_X_train, columns=encoded_feature_names)
df_final["price"] = df_cleaned.price
df_final.head()


# Assuming 'df_cleaned' is the original DataFrame and 'df_final' is the transformed DataFrame
# Extract the 'clean_title' binary feature from the original DataFrame
clean_title = df_cleaned['clean_title']

# Add the 'clean_title' column to df_final (after pipeline transformation)
df_final['clean_title'] = clean_title

# Now df_final includes the transformed features along with the 'clean_title'



df_final.info()


# Feature selection
# correlation analysis
corrs = df_final.corr()
corrs


plt.figure(figsize=(12,8))
sns.heatmap(corrs, annot=True)
plt.title("Correlation Heatmap")
plt.show()


abs(corrs.price).sort_values(ascending=False)


df_final["log_price"] = np.log1p(df_final["price"])


plt.figure(figsize=(12, 5))

# Original Price Distribution
plt.subplot(1, 2, 1)
sns.histplot(df_final["price"], bins=30, kde=True)
plt.title("Original Price Distribution")

# Log-Transformed Price Distribution
plt.subplot(1, 2, 2)
sns.histplot(df_final["log_price"], bins=30, kde=True)
plt.title("Log-Transformed Price Distribution")

plt.show()



# Step 1: Get the Data
X = df_final.drop(columns=["price","log_price"])
y = df_final.log_price

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # stratify ensures same % of the target classes in the train/test

print(X.shape)
print(X_train.shape)
print(X_test.shape)

# Combine the features and target variable for training and test data
train_data = pd.concat([X_train, y_train], axis=1)
test_data = pd.concat([X_test, y_test], axis=1)

# Save the DataFrames to CSV files
train_data.to_csv('../Resources/train_data.csv', index=False)
test_data.to_csv('../Resources/test_data.csv', index=False)


# Function for Regression
def doRegression(model, X_train, X_test, y_train, y_test):
    # Step 3: Fit the model
    model.fit(X_train, y_train)
    
    # Step 4: Evaluate the model
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)

    # Generate metrics TRAIN
    train_r2 = r2_score(y_train, train_preds)
    train_mse = mean_squared_error(y_train, train_preds)
    train_mae = mean_absolute_error(y_train, train_preds)
    train_rmse = np.sqrt(train_mse)
    
    train_results = f"""TRAIN METRICS
    R2: {train_r2}
    MSE: {train_mse}
    RMSE: {train_rmse}
    MAE: {train_mae}
    """
    
    print(train_results)

    # Generate metrics TEST
    test_r2 = r2_score(y_test, test_preds)
    test_mse = mean_squared_error(y_test, test_preds)
    test_mae = mean_absolute_error(y_test, test_preds)
    test_rmse = np.sqrt(test_mse)
    
    test_results = f"""TEST METRICS
    R2: {test_r2}
    MSE: {test_mse}
    RMSE: {test_rmse}
    MAE: {test_mae}
    """
    
    print(test_results)

    # VISUALIZE TEST RESULTS
    # Predicted vs Actual Plot
    plt.scatter(y_test, test_preds)
    plt.plot(y_test, y_test)
    plt.title("Predicted vs Actual Plot")
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.show()

    # Residual Plot
    resids = test_preds - y_test
    plt.scatter(test_preds, resids)
    plt.hlines(0, min(test_preds), max(test_preds))
    plt.title("Residual Plot")
    plt.xlabel("Predictions")
    plt.ylabel("Residuals")
    plt.show()


# Function for Regression
def doRegression(model, X_train, X_test, y_train, y_test):
    # Step 3: Fit the model
    model.fit(X_train, y_train)
    
    # Step 4: Evaluate the model
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)

    # Generate metrics TRAIN
    train_r2 = r2_score(y_train, train_preds)
    train_mse = mean_squared_error(y_train, train_preds)
    train_mae = mean_absolute_error(y_train, train_preds)
    train_rmse = np.sqrt(train_mse)
    
    train_results = f"""TRAIN METRICS
    R2: {train_r2}
    MSE: {train_mse}
    RMSE: {train_rmse}
    MAE: {train_mae}
    """
    
    print(train_results)

    # Generate metrics TEST
    test_r2 = r2_score(y_test, test_preds)
    test_mse = mean_squared_error(y_test, test_preds)
    test_mae = mean_absolute_error(y_test, test_preds)
    test_rmse = np.sqrt(test_mse)
    
    test_results = f"""TEST METRICS
    R2: {test_r2}
    MSE: {test_mse}
    RMSE: {test_rmse}
    MAE: {test_mae}
    """
    
    print(test_results)

    # VISUALIZE TEST RESULTS
    # Predicted vs Actual Plot
    plt.scatter(y_test, test_preds)
    plt.plot(y_test, y_test)
    plt.title("Predicted vs Actual Plot")
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.show()

    # Residual Plot
    resids = test_preds - y_test
    plt.scatter(test_preds, resids)
    plt.hlines(0, min(test_preds), max(test_preds))
    plt.title("Residual Plot")
    plt.xlabel("Predictions")
    plt.ylabel("Residuals")
    plt.show()


# Step 2: Init the model
lr = LinearRegression()

doRegression(lr, X_train, X_test, y_train, y_test)


# Step 2: Init the Model
ridge = Ridge()

# Do Machine Learning
doRegression(ridge, X_train, X_test, y_train, y_test)


 # Step 2: Init the Model
lasso = Lasso()

# Do Machine Learning
doRegression(lasso, X_train, X_test, y_train, y_test)


# Step 2: Init the Model
rf = RandomForestRegressor(random_state=42)

# Do Machine Learning
doRegression(rf, X_train, X_test, y_train, y_test)


# Step 2: Init the Model
gb = GradientBoostingRegressor(random_state=42)

# Do Machine Learning
doRegression(gb, X_train, X_test, y_train, y_test)



# Step 2: Init the Model
xgb = XGBRegressor(random_state=42)

# Do Machine Learning
doRegression(xgb, X_train, X_test, y_train, y_test)



# Step 2: Initialize the CatBoost model
catboost_model = CatBoostRegressor(random_state=42)

# Step 3: Call the doRegression function 
doRegression(catboost_model, X_train, X_test, y_train, y_test)


fi = pd.DataFrame(list(zip(X.columns, catboost_model.feature_importances_)), columns=["Feature", "Importance"])
fi = fi.sort_values(by="Importance", ascending=False)
fi


# Save model
model = pickle.dump(xgb, open("model_pipeline.pkl", 'wb'))
# wb = write binary for the h5 file



print(X.columns)
